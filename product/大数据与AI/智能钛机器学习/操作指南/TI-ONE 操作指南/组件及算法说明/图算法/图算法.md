

## [2.0]HyperAnf
估计网络的平均半径，参考论文 [HyperANF: Approximating the Neighbourhood Function of Very Large Graphs on a Budget](https://arxiv.org/pdf/1011.5599.pdf) 开发，详细细节请看论文。

#### 输入
- 输入数据路径：输入文件所在路径，无权网络数据, 数据格式为两列 `srcId(long) | dstId(long)`, 其中`|`为分隔符，分隔字段表示空白符或者逗号等
- 输入文件类型：格式包括以下三种：
  - csv：csv 文件
    - 输入数据包含 header 信息
    - 输入数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet

#### 输出
- 输出数据路径：输出文件所在路径。
- 输出数据格式：格式包括以下两种：
    - csv：csv 文件。
         - 输出数据包含 header 信息。
         - 输出数据分割符：主要包括逗号、空格、分号、星号等分割符。
    - parquet：列式存储格式 parquet。

算法结果保存路径，共两列，其中第一列为 round 值，第二列为 anf值；其中 round = -1对应的 anf 为最终估计值。

**参数说明**
- src：源节点列。
- dst：目标节点列。
- numPartition：分区数。
- maxIter：最大迭代次数。

## [2.0]CommonFriends
计算两个好友的共同好友数，某种程度上可以刻画两个节点之间的紧密程度
- **算法 IO 参数**
- 输入路径： 无权网络, 数据格式为两列 `srcId(long) | dstId(long)`, 其中`|`为分隔符，表示空白符或者逗号
  - 结果输出路径：输出的格式为两列 srcId(long) | dstId(long) | count(int)
  - 分区数: Spark的数据分区数, 一般为使用的总核数的2 - 10倍


- **资源参数**
 - num-executors： 使用多少个 Spark 节点
 - driver-memory： Spark driver 的内存大小
 - executor-cores：每个 Spark 节点使用多少个 core
 - executor-memory：每个 Spark 节点使用的内存大小
 - spark-conf：Spark 的其他参数。 特别的，由于权限原因，需要用户额外提供 ugi 参数 spark.hadoop.hadoop.job.ugi=用户名：密码

#### 输入
- 输入数据路径：输入文件所在路径
- 输入文件类型：格式包括以下三种：
  - csv：csv 文件
    - 输入数据包含 header 信息
    - 输入数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet

#### 输出
- 输出数据路径：输出文件所在路径。
- 输出数据格式：格式包括以下三种：
  - csv：csv 文件
    - 输出数据包含 header 信息
    - 输出数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet

算法结果保存路径，共三列，其中第一列为源点 ID，第二列为终点 ID，第三列为共同好友数.

- **资源参数**
 - num-executors： 使用多少个 Spark 节点
 - driver-memory： Spark driver 的内存大小
 - executor-cores：每个 Spark 节点使用多少个 core
 - executor-memory：每个 Spark 节点使用的内存大小
 - spark-conf：Spark 的其他参数。 特别的，由于权限原因，需要用户额外提供 ugi 参数 spark.hadoop.hadoop.job.ugi=用户名：密码

#### 参数说明
- src：源节点列。
- dst：目标节点列。
- numPartition：分区数。

## [2.0]PageRank
**输入**
- 输入数据路径：输入文件所在路径。对不带权的网络数据格式为srcId(long) | dstId(long)`, 对带权网络，数据格式为`srcId(long) | dstId(long) | weight(float)`, 其中`|为分隔符，表示表示空白符或者逗号等
- 输入文件类型：格式包括以下三种：
  - csv：csv 文件
    - 输入数据包含 header 信息
    - 输入数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet

#### 输出
- 输出数据路径：输出文件所在路径。
- 输出数据格式：格式包括以下三种：
  - csv：csv 文件
    - 输出数据包含 header 信息
    - 输出数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet

算法结果保存路径，共两列，其中第一列为节点 ID，第二列为节点对应的社区 ID。社区 ID 相同表示属于同一个社区。

- **资源参数**
 - num-executors： 使用多少个 Spark 节点
 - driver-memory： Spark driver 的内存大小
 - executor-cores：每个 Spark 节点使用多少个 core
 - executor-memory：每个 Spark 节点使用的内存大小
 - spark-conf：Spark 的其他参数。 特别的，由于权限原因，需要用户额外提供 ugi 参数 spark.hadoop.hadoop.job.ugi=用户名：密码

#### 参数说明
- src：源节点列。
- dst：目标节点列。
- numPartition：分区数。
- maxIter：最大迭代次数。
- tol：差值迭代参数，在所有节点的前后 Rank 插值低于该值的时候，算法提早终止。

## [2.0]LPA
LPA（Label Propagation Algorithm）是最简单的社区发现算法，通过标签扩散发掘网络的社区关系。

#### 输入
- 输出数据路径：输出文件所在路径。
- 输入文件类型：格式包括以下三种：
  - csv：csv 文件
    - 输入数据包含 header 信息
    - 输入数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet
  
#### 输出
- 输出数据路径：输出文件所在路径。
- 输出数据格式：格式包括以下三种：
  - csv：csv 文件
    - 输出数据包含 header 信息
    - 输出数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet
  
算法结果保存路径，共两列，其中第一列为节点 ID，第二列为节点对应的社区 ID。社区 ID 相同表示属于同一个社区。
  

- **资源参数**
 - num-executors： 使用多少个 Spark 节点
 - driver-memory： Spark driver 的内存大小
 - executor-cores：每个 Spark 节点使用多少个 core
 - executor-memory：每个 Spark 节点使用的内存大小
 - spark-conf：Spark 的其他参数。 特别的，由于权限原因，需要用户额外提供 ugi 参数 spark.hadoop.hadoop.job.ugi=用户名：密码

#### 参数说明
- src：源节点列。
- dst：目标节点列。
- numPartition：分区数。

## [2.0]EffectiveSize
EffectiveSize 是由结构空洞理论得到的网络度量指标，是 ego-network 中节点的重要衡量指标。

#### 输入
- 输入数据路径：输入文件所在路径。
- 输入文件类型：格式包括以下三种：
  - csv：csv 文件
    - 输入数据包含 header 信息
    - 输入数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet
  
#### 输出
- 输出数据路径：输出文件所在路径。
- 输出数据格式：格式包括以下三种：
  - csv：csv 文件
    - 输出数据包含 header 信息
    - 输出数据分割符：主要包括逗号、空格、分号、星号等分割符
  - text：文本文件
  - parquet：列式存储格式 parquet

算法结果保存路径，共三列，其中第一列为节点 ID，第二列为 effectiveSize 值，第三列为 redundancyCol 值。

- **资源参数**
 - num-executors： 使用多少个 Spark 节点
 - driver-memory： Spark driver 的内存大小
 - executor-cores：每个 Spark 节点使用多少个 core
 - executor-memory：每个 Spark 节点使用的内存大小
 - spark-conf：Spark 的其他参数。 特别的，由于权限原因，需要用户额外提供 ugi 参数 spark.hadoop.hadoop.job.ugi=用户名：密码

#### 参数说明
- src：源节点列。
- dst：目标节点列。
- numPartition：分区数。
